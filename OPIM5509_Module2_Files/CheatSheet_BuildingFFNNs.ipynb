{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/drdave-teaching/OPIM5509Files/blob/main/OPIM5509_Module2_Files/CheatSheet_BuildingFFNNs.ipynb)"
      ],
      "metadata": {
        "id": "BVwTvOMGOTbT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbsOJMFoRQTz"
      },
      "source": [
        "# Cheat Sheet: Building FFNNs\n",
        "---------------------------------------\n",
        "**Dr. Dave Wanik - University of Connecticut**\n",
        "\n",
        "A lot of students want to know how many layers and hidden units to use in their neural networks. The easy/annoying answer is 'it depends on the problem', which can be frustrating to a Deep Learning newbie. However, there are some rules of thumb and general guidelines that may be useful to you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHaskK5bRm0W"
      },
      "source": [
        "## Strategy 1: One layer, same number of hidden nodes as input data features.\n",
        "\n",
        "This is an easy one to understand - remember, you are making intermediate/temporary, nonlinear representations of data. So, if you have 10 input data features, you can create 10 intermediate, nonlinear representations of data.\n",
        "\n",
        "This makes sense - your 10 inputs can be combined in such a way that you make 10 new things that are useful for prediction. Each one of the nodes in the hidden layer is a different nonlinear combination of inputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train.shape[1] = 13 features"
      ],
      "metadata": {
        "id": "xtjftzeaSMMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model!\n",
        "model = Sequential()\n",
        "model.add(Dense(13, input_shape=(X_train.shape[1],), activation='relu')) # (features,)\n",
        "model.add(Dense(1, activation='linear')) # output node\n",
        "model.summary() # see what your model looks like"
      ],
      "metadata": {
        "id": "MwCIidh4SI3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGKl0wLFRwBN"
      },
      "source": [
        "## Strategy 2: One layer, two times the number of input data features.\n",
        "\n",
        "If you have 10 input data features, you create a single layer with 20 hidden units. You are making 20 nonlinear representations of data from your 10 input features. This seems to be believeable to me.\n",
        "\n",
        "Perhaps you could also try three or five times the input data features - but realize what you are doing, you are going to slow down training because you have to tune your network and get it to converge."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model!\n",
        "model = Sequential()\n",
        "model.add(Dense(26, input_shape=(X_train.shape[1],), activation='relu')) # (features,)\n",
        "model.add(Dense(1, activation='linear')) # output node\n",
        "model.summary() # see what your model looks like"
      ],
      "metadata": {
        "id": "CRGB0fbNSVae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjf-XLeRS8c_"
      },
      "source": [
        "## Strategy 3: Two or more layers, same number of hidden units as input data features.\n",
        "\n",
        "Recall WHY we are using multiple layers - multiple layers mean that you can have deeper representations of data. This table is not the ultimate authority on neural networks, but it will get you thinking the right way.\n",
        "\n",
        "**Table:** Determining the Number of Hidden Layers\n",
        "\n",
        "Num Hidden Layers|\tResult|\n",
        "---|---|\n",
        "none|\tOnly capable of representing linear separable functions or decisions.\n",
        "1\t|Can approximate any function that contains a continuous mapping from one finite space to another.|\n",
        "2\t|Can represent an arbitrary decision boundary to arbitrary accuracy with rational activation functions and can approximate any smooth mapping to any accuracy.\n",
        ">2\t|Additional layers can learn complex representations (sort of automatic feature engineering) for layers.\n",
        "\n",
        "We see this all the time in the Chollet book. Just keep repeating the layer size!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model!\n",
        "model = Sequential()\n",
        "model.add(Dense(13, input_shape=(X_train.shape[1],), activation='relu')) # (features,)\n",
        "model.add(Dense(13, activation='relu')) # output node\n",
        "model.add(Dense(13, activation='relu')) # output node\n",
        "model.add(Dense(13, activation='relu')) # output node\n",
        "model.add(Dense(13, activation='relu')) # output node\n",
        "model.add(Dense(13, activation='relu')) # output node\n",
        "model.add(Dense(1, activation='linear')) # output node\n",
        "model.summary() # see what your model looks like"
      ],
      "metadata": {
        "id": "FFwqEKIvSdrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVxXD2r_Ux89"
      },
      "source": [
        "## Strategy 4: Many layers, decreasing hidden nodes by half.\n",
        "\n",
        "Imagine you have a first layer with 50 hidden units - your second layer would have 25 hidden units, then 12, then 6. Just keep dividing by 2. This is what I mean by 'information funnel' - because the shape goes from wide to narrow, left to right. You are creating new representations as you go along - and the nonlinear nuggets you create at the end will be nonlinearly combined to predict the output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model!\n",
        "model = Sequential()\n",
        "model.add(Dense(1000, input_shape=(X_train.shape[1],), activation='relu')) # (features,)\n",
        "model.add(Dense(500, activation='relu')) # output node\n",
        "model.add(Dense(250, activation='relu')) # output node\n",
        "model.add(Dense(125, activation='relu')) # output node\n",
        "model.add(Dense(60, activation='relu')) # output node\n",
        "model.add(Dense(30, activation='relu')) # output node\n",
        "model.add(Dense(15, activation='relu')) # output node\n",
        "model.add(Dense(1, activation='linear')) # output node\n",
        "model.summary() # see what your model looks like"
      ],
      "metadata": {
        "id": "u5yNgEQ4S2sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyCpagHpV75E"
      },
      "source": [
        "## [Optional] Strategy 5: A systematic grid search\n",
        "This is a bit advanced, but do as Brownlee does and try to tune everything at once in a loop!\n",
        "\n",
        "* https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
        "\n",
        "PS also check out how to build your own NN from scratch - just as FYI - use the Sequential() API from Keras to make things easier!\n",
        "* https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEOn0Df_TGhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfWyMuzTVgx1"
      },
      "source": [
        "## Bells and Whistles\n",
        "You should always be using:\n",
        "* Dropout between 0.1 and 0.5\n",
        "* Early stopping with a suitable patience or other stopping criterion (min decrease in error).\n",
        "* 'relu' or 'tanh' activation function in every layer (except for output node - there you have to use the appropriate 'linear', 'sigmoid' or 'softmax'). You need to know the difference between all of these!\n",
        "* batch size - use full, stochastic or batch gradient descent and see how that affects learning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (new!) Bayesian hyperparameter tuning with Optuna\n",
        "* https://www.kaggle.com/code/mistag/keras-model-tuning-with-optuna\n",
        "\n",
        "^ This uses Optuna to hyperparameter tune a Recurrent Neural Network\n",
        "\n"
      ],
      "metadata": {
        "id": "qI38UoQ4pgrb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnVYIj_SReW-"
      },
      "source": [
        "# Resources\n",
        "* https://www.heatonresearch.com/2017/06/01/hidden-layers.html\n",
        "* https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af\n",
        "* https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/"
      ]
    }
  ]
}