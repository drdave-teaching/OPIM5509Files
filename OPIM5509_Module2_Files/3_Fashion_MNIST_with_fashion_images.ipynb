{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/drdave-teaching/OPIM5509Files/blob/main/OPIM5509_Module2_Files/3_Fashion_MNIST_with_fashion_images.ipynb)"
      ],
      "metadata": {
        "id": "3zzINL5IchTY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80y2_zhbjcd9"
      },
      "source": [
        "# A first look at a neural network with Fashion MNIST\n",
        "-----------------------\n",
        "**Dr. Dave Wanik**\n",
        "\n",
        "This notebook contains the code samples found in Chapter 2, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "We will now take a look at a first concrete example of a neural network, which makes use of the Python library Keras to learn to classify\n",
        "hand-written digits. Unless you already have experience with Keras or similar libraries, you will not understand everything about this\n",
        "first example right away. You probably haven't even installed Keras yet. Don't worry, that is perfectly fine. In the next chapter, we will\n",
        "review each element in our example and explain them in detail. So don't worry if some steps seem arbitrary or look like magic to you!\n",
        "We've got to start somewhere.\n",
        "\n",
        "The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10\n",
        "categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been\n",
        "around for almost as long as the field itself and has been very intensively studied. It's a set of 60,000 training images, plus 10,000 test\n",
        "images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of \"solving\" MNIST\n",
        "as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. As you become a machine\n",
        "learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3hpfsz8jcd9"
      },
      "source": [
        "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXP0bOStjcd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3d50e4-dae9-4c69-c90e-124f394a08dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqb3khkljceC"
      },
      "source": [
        "`train_images` and `train_labels` form the \"training set\", the data that the model will learn from. The model will then be tested on the\n",
        "\"test set\", `test_images` and `test_labels`. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging\n",
        "from 0 to 9. There is a one-to-one correspondence between the images and the labels.\n",
        "\n",
        "Let's have a look at the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOa0EUQqjceD",
        "outputId": "560bea52-5d84-450d-f039-a8210556b453"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIcI5aa0jceF",
        "outputId": "6cbcb12d-cfe0-4985-d9a0-29ef6faa1a3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n8Hqgi1jceI",
        "outputId": "68322521-ea7e-41b9-d4d2-eb7b584e4b07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJArl17GXgjr",
        "outputId": "a12cf974-71d3-4089-8d1c-8923aa23ada9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9    6000\n",
              "0    6000\n",
              "3    6000\n",
              "2    6000\n",
              "7    6000\n",
              "5    6000\n",
              "1    6000\n",
              "6    6000\n",
              "4    6000\n",
              "8    6000\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tmp = train_labels\n",
        "import pandas as pd\n",
        "tmp = pd.DataFrame(tmp)\n",
        "tmp[0].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OydGZhFglx7Z"
      },
      "source": [
        "## Look at one image...\n",
        "Just to get a feel for the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YP07k0Q5j8mH",
        "outputId": "10ef2299-eb0b-4fa9-e58f-217a71b624d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    0    1    2    3    4    5    6    7    8    9   ...   18   19   20   21  \\\n",
              "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "3    0    0    0    0    0    0    0    0    0    0  ...    0    1    4    0   \n",
              "4    0    0    0    0    0    0    0    0    0    0  ...   54    0    0    0   \n",
              "5    0    0    0    0    0    0    0    0    0    0  ...  144  123   23    0   \n",
              "6    0    0    0    0    0    0    0    0    0    0  ...  107  156  161  109   \n",
              "7    0    0    0    0    0    0    0    0    0    0  ...  216  163  127  121   \n",
              "8    0    0    0    0    0    0    0    0    0    1  ...  223  223  215  213   \n",
              "9    0    0    0    0    0    0    0    0    0    0  ...  235  227  224  222   \n",
              "10   0    0    0    0    0    0    0    0    0    0  ...  180  212  210  211   \n",
              "11   0    0    0    0    0    0    0    0    0    1  ...  169  227  208  218   \n",
              "12   0    0    0    0    0    0    0    0    0    0  ...  198  221  215  213   \n",
              "13   0    0    0    0    0    0    0    0    0    4  ...  232  213  218  223   \n",
              "14   0    0    1    4    6    7    2    0    0    0  ...  222  221  216  223   \n",
              "15   0    3    0    0    0    0    0    0    0   62  ...  211  218  224  223   \n",
              "16   0    0    0    0   18   44   82  107  189  228  ...  224  234  176  188   \n",
              "17   0   57  187  208  224  221  224  208  204  214  ...  255  255  221  234   \n",
              "18   3  202  228  224  221  211  211  214  205  205  ...  188  154  191  210   \n",
              "19  98  233  198  210  222  229  229  234  249  220  ...  168  219  221  215   \n",
              "20  75  204  212  204  193  205  211  225  216  185  ...  239  223  218  212   \n",
              "21  48  203  183  194  213  197  185  190  194  192  ...  199  206  186  181   \n",
              "22   0  122  219  193  179  171  183  196  204  210  ...  195  191  198  192   \n",
              "23   0    0   74  189  212  191  175  172  175  181  ...  210  210  211  188   \n",
              "24   2    0    0    0   66  200  222  237  239  242  ...  182  182  181  176   \n",
              "25   0    0    0    0    0    0    0   40   61   44  ...    0    0    0    0   \n",
              "26   0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "27   0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "\n",
              "     22   23   24   25   26   27  \n",
              "0     0    0    0    0    0    0  \n",
              "1     0    0    0    0    0    0  \n",
              "2     0    0    0    0    0    0  \n",
              "3     0    0    0    1    1    0  \n",
              "4     1    3    4    0    0    3  \n",
              "5     0    0    0   12   10    0  \n",
              "6    64   23   77  130   72   15  \n",
              "7   122  146  141   88  172   66  \n",
              "8   164  127  123  196  229    0  \n",
              "9   224  221  223  245  173    0  \n",
              "10  213  223  220  243  202    0  \n",
              "11  224  212  226  197  209   52  \n",
              "12  222  220  245  119  167   56  \n",
              "13  234  217  217  209   92    0  \n",
              "14  229  215  218  255   77    0  \n",
              "15  219  215  224  244  159    0  \n",
              "16  250  248  233  238  215    0  \n",
              "17  221  211  220  232  246    0  \n",
              "18  204  209  222  228  225    0  \n",
              "19  217  223  223  224  229   29  \n",
              "20  209  222  220  221  230   67  \n",
              "21  177  172  181  205  206  115  \n",
              "22  176  156  167  177  210   92  \n",
              "23  188  194  192  216  170    0  \n",
              "24  166  168   99   58    0    0  \n",
              "25    0    0    0    0    0    0  \n",
              "26    0    0    0    0    0    0  \n",
              "27    0    0    0    0    0    0  \n",
              "\n",
              "[28 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-200f3a80-01bb-461d-9ce4-409a46a9296e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>144</td>\n",
              "      <td>123</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>107</td>\n",
              "      <td>156</td>\n",
              "      <td>161</td>\n",
              "      <td>109</td>\n",
              "      <td>64</td>\n",
              "      <td>23</td>\n",
              "      <td>77</td>\n",
              "      <td>130</td>\n",
              "      <td>72</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>216</td>\n",
              "      <td>163</td>\n",
              "      <td>127</td>\n",
              "      <td>121</td>\n",
              "      <td>122</td>\n",
              "      <td>146</td>\n",
              "      <td>141</td>\n",
              "      <td>88</td>\n",
              "      <td>172</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>223</td>\n",
              "      <td>223</td>\n",
              "      <td>215</td>\n",
              "      <td>213</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>123</td>\n",
              "      <td>196</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>235</td>\n",
              "      <td>227</td>\n",
              "      <td>224</td>\n",
              "      <td>222</td>\n",
              "      <td>224</td>\n",
              "      <td>221</td>\n",
              "      <td>223</td>\n",
              "      <td>245</td>\n",
              "      <td>173</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>180</td>\n",
              "      <td>212</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>213</td>\n",
              "      <td>223</td>\n",
              "      <td>220</td>\n",
              "      <td>243</td>\n",
              "      <td>202</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>169</td>\n",
              "      <td>227</td>\n",
              "      <td>208</td>\n",
              "      <td>218</td>\n",
              "      <td>224</td>\n",
              "      <td>212</td>\n",
              "      <td>226</td>\n",
              "      <td>197</td>\n",
              "      <td>209</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>198</td>\n",
              "      <td>221</td>\n",
              "      <td>215</td>\n",
              "      <td>213</td>\n",
              "      <td>222</td>\n",
              "      <td>220</td>\n",
              "      <td>245</td>\n",
              "      <td>119</td>\n",
              "      <td>167</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>232</td>\n",
              "      <td>213</td>\n",
              "      <td>218</td>\n",
              "      <td>223</td>\n",
              "      <td>234</td>\n",
              "      <td>217</td>\n",
              "      <td>217</td>\n",
              "      <td>209</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>222</td>\n",
              "      <td>221</td>\n",
              "      <td>216</td>\n",
              "      <td>223</td>\n",
              "      <td>229</td>\n",
              "      <td>215</td>\n",
              "      <td>218</td>\n",
              "      <td>255</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>...</td>\n",
              "      <td>211</td>\n",
              "      <td>218</td>\n",
              "      <td>224</td>\n",
              "      <td>223</td>\n",
              "      <td>219</td>\n",
              "      <td>215</td>\n",
              "      <td>224</td>\n",
              "      <td>244</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>44</td>\n",
              "      <td>82</td>\n",
              "      <td>107</td>\n",
              "      <td>189</td>\n",
              "      <td>228</td>\n",
              "      <td>...</td>\n",
              "      <td>224</td>\n",
              "      <td>234</td>\n",
              "      <td>176</td>\n",
              "      <td>188</td>\n",
              "      <td>250</td>\n",
              "      <td>248</td>\n",
              "      <td>233</td>\n",
              "      <td>238</td>\n",
              "      <td>215</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>57</td>\n",
              "      <td>187</td>\n",
              "      <td>208</td>\n",
              "      <td>224</td>\n",
              "      <td>221</td>\n",
              "      <td>224</td>\n",
              "      <td>208</td>\n",
              "      <td>204</td>\n",
              "      <td>214</td>\n",
              "      <td>...</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>221</td>\n",
              "      <td>234</td>\n",
              "      <td>221</td>\n",
              "      <td>211</td>\n",
              "      <td>220</td>\n",
              "      <td>232</td>\n",
              "      <td>246</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>202</td>\n",
              "      <td>228</td>\n",
              "      <td>224</td>\n",
              "      <td>221</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>214</td>\n",
              "      <td>205</td>\n",
              "      <td>205</td>\n",
              "      <td>...</td>\n",
              "      <td>188</td>\n",
              "      <td>154</td>\n",
              "      <td>191</td>\n",
              "      <td>210</td>\n",
              "      <td>204</td>\n",
              "      <td>209</td>\n",
              "      <td>222</td>\n",
              "      <td>228</td>\n",
              "      <td>225</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>98</td>\n",
              "      <td>233</td>\n",
              "      <td>198</td>\n",
              "      <td>210</td>\n",
              "      <td>222</td>\n",
              "      <td>229</td>\n",
              "      <td>229</td>\n",
              "      <td>234</td>\n",
              "      <td>249</td>\n",
              "      <td>220</td>\n",
              "      <td>...</td>\n",
              "      <td>168</td>\n",
              "      <td>219</td>\n",
              "      <td>221</td>\n",
              "      <td>215</td>\n",
              "      <td>217</td>\n",
              "      <td>223</td>\n",
              "      <td>223</td>\n",
              "      <td>224</td>\n",
              "      <td>229</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>75</td>\n",
              "      <td>204</td>\n",
              "      <td>212</td>\n",
              "      <td>204</td>\n",
              "      <td>193</td>\n",
              "      <td>205</td>\n",
              "      <td>211</td>\n",
              "      <td>225</td>\n",
              "      <td>216</td>\n",
              "      <td>185</td>\n",
              "      <td>...</td>\n",
              "      <td>239</td>\n",
              "      <td>223</td>\n",
              "      <td>218</td>\n",
              "      <td>212</td>\n",
              "      <td>209</td>\n",
              "      <td>222</td>\n",
              "      <td>220</td>\n",
              "      <td>221</td>\n",
              "      <td>230</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>48</td>\n",
              "      <td>203</td>\n",
              "      <td>183</td>\n",
              "      <td>194</td>\n",
              "      <td>213</td>\n",
              "      <td>197</td>\n",
              "      <td>185</td>\n",
              "      <td>190</td>\n",
              "      <td>194</td>\n",
              "      <td>192</td>\n",
              "      <td>...</td>\n",
              "      <td>199</td>\n",
              "      <td>206</td>\n",
              "      <td>186</td>\n",
              "      <td>181</td>\n",
              "      <td>177</td>\n",
              "      <td>172</td>\n",
              "      <td>181</td>\n",
              "      <td>205</td>\n",
              "      <td>206</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>219</td>\n",
              "      <td>193</td>\n",
              "      <td>179</td>\n",
              "      <td>171</td>\n",
              "      <td>183</td>\n",
              "      <td>196</td>\n",
              "      <td>204</td>\n",
              "      <td>210</td>\n",
              "      <td>...</td>\n",
              "      <td>195</td>\n",
              "      <td>191</td>\n",
              "      <td>198</td>\n",
              "      <td>192</td>\n",
              "      <td>176</td>\n",
              "      <td>156</td>\n",
              "      <td>167</td>\n",
              "      <td>177</td>\n",
              "      <td>210</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>189</td>\n",
              "      <td>212</td>\n",
              "      <td>191</td>\n",
              "      <td>175</td>\n",
              "      <td>172</td>\n",
              "      <td>175</td>\n",
              "      <td>181</td>\n",
              "      <td>...</td>\n",
              "      <td>210</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>194</td>\n",
              "      <td>192</td>\n",
              "      <td>216</td>\n",
              "      <td>170</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>200</td>\n",
              "      <td>222</td>\n",
              "      <td>237</td>\n",
              "      <td>239</td>\n",
              "      <td>242</td>\n",
              "      <td>...</td>\n",
              "      <td>182</td>\n",
              "      <td>182</td>\n",
              "      <td>181</td>\n",
              "      <td>176</td>\n",
              "      <td>166</td>\n",
              "      <td>168</td>\n",
              "      <td>99</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>61</td>\n",
              "      <td>44</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 28 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-200f3a80-01bb-461d-9ce4-409a46a9296e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-200f3a80-01bb-461d-9ce4-409a46a9296e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-200f3a80-01bb-461d-9ce4-409a46a9296e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# look at one image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# subset first 'image'\n",
        "x = train_images[0]\n",
        "# convert to pandas dataframe for easy viewing...\n",
        "x = pd.DataFrame(x)\n",
        "print(x.shape) # 28 rows and 28 columns\n",
        "# you can confirm\n",
        "print(train_labels[0])\n",
        "x # it's the number 9!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFO0uvafjceL"
      },
      "source": [
        "Let's have a look at the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6SYScQcjceM",
        "outputId": "d464638e-ec4f-49be-ed62-1b9b33068833"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tdz8MKpjceP",
        "outputId": "d26f41ac-98e0-4377-859d-6c6dbc83829f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXUC6KQajceU",
        "outputId": "8ad5a4ad-1b93-45e3-ecef-57ff70572b4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, 1, 6, 1, 4, 6, 5, 7], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_labels[0:10] # just a sample of them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v72T0A3dZNmD",
        "outputId": "85c970ec-68f8-4b1d-e30e-031bf1b2ffdf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9    1000\n",
              "2    1000\n",
              "1    1000\n",
              "6    1000\n",
              "4    1000\n",
              "5    1000\n",
              "7    1000\n",
              "3    1000\n",
              "8    1000\n",
              "0    1000\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# check out value counts\n",
        "tmp = test_labels\n",
        "import pandas as pd\n",
        "tmp = pd.DataFrame(tmp)\n",
        "tmp[0].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQtiTsOIw-Dp"
      },
      "outputs": [],
      "source": [
        "# making a copy for later (logistic regression)\n",
        "copy_train_labels = train_labels\n",
        "copy_test_labels = test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS8WwVfLjceX"
      },
      "source": [
        "Our workflow will be as follow: first we will present our neural network with the training data, `train_images` and `train_labels`. The\n",
        "network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_images`, and we\n",
        "will verify if these predictions match the labels from `test_labels`.\n",
        "\n",
        "Let's build our network -- again, remember that you aren't supposed to understand everything about this example just yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bPKpXZJjceX",
        "outputId": "1b9c4205-717e-4bbe-c04f-6ee358080eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               51300     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 454,230\n",
            "Trainable params: 454,230\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(Dense(512, activation='relu', input_shape=(28 * 28,))) # (features, )\n",
        "network.add(Dropout(0.2))\n",
        "network.add(Dense(100, activation='relu'))\n",
        "network.add(Dropout(0.2))\n",
        "network.add(Dense(10, activation='softmax'))\n",
        "network.summary()\n",
        "\n",
        "# note: input shape is the trickiest part IMHO...\n",
        "# think of it as (number of columns, number of samples)\n",
        "# number of samples is ALWAYS left blank (you'll set the batch size later on!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RpleqSajceb"
      },
      "source": [
        "\n",
        "The core building block of neural networks is the \"layer\", a data-processing module which you can conceive as a \"filter\" for data. Some\n",
        "data comes in, and comes out in a more useful form. Precisely, layers extract _representations_ out of the data fed into them -- hopefully\n",
        "representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers\n",
        "which will implement a form of progressive \"data distillation\". A deep learning model is like a sieve for data processing, made of a\n",
        "succession of increasingly refined data filters -- the \"layers\".\n",
        "\n",
        "Here our network consists of a sequence of two `Dense` layers, which are densely-connected (also called \"fully-connected\") neural layers.\n",
        "The second (and last) layer is a 10-way \"softmax\" layer, which means it will return an array of 10 probability scores (summing to 1). Each\n",
        "score will be the probability that the current digit image belongs to one of our 10 digit classes.\n",
        "\n",
        "To make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n",
        "\n",
        "* A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be\n",
        "able to steer itself in the right direction.\n",
        "* An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
        "* Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly\n",
        "classified).\n",
        "\n",
        "The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHzWKWCKjcec"
      },
      "outputs": [],
      "source": [
        "network.compile(optimizer='rmsprop', # you could use rmsprop or Adam - both are good!\n",
        "                loss='categorical_crossentropy', # many categories - you need to use 'categorical_crossentropy' and NOT 'binary_crossentropy'\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArMRcF1Kjcef"
      },
      "source": [
        "\n",
        "Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in\n",
        "the `[0, 1]` interval. Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with\n",
        "values in the `[0, 255]` interval. We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7K3v0n9aJa-",
        "outputId": "26aa1ec8-9a43-4fa9-931c-cec22de6d6e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "28*28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU_x8YWXjceg"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28)) # 60K rows, with 784 columns... wow!\n",
        "train_images = train_images.astype('float32') / 255 # here we are scaling by max pixel value (where white =255, black = 0)\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gil6-5rsmR61",
        "outputId": "488101f8-cece-4444-da23-ccbeea1ab43a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.00392157, 0.        , 0.        , 0.05098039,\n",
              "       0.28627452, 0.        , 0.        , 0.00392157, 0.01568628,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
              "       0.        , 0.14117648, 0.53333336, 0.49803922, 0.24313726,\n",
              "       0.21176471, 0.        , 0.        , 0.        , 0.00392157,\n",
              "       0.01176471, 0.01568628, 0.        , 0.        , 0.01176471,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
              "       0.8       , 0.6901961 , 0.5254902 , 0.5647059 , 0.48235294,\n",
              "       0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.04705882, 0.03921569, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.60784316, 0.9254902 , 0.8117647 ,\n",
              "       0.69803923, 0.41960785, 0.6117647 , 0.6313726 , 0.42745098,\n",
              "       0.2509804 , 0.09019608, 0.3019608 , 0.50980395, 0.28235295,\n",
              "       0.05882353, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.00392157, 0.        , 0.27058825,\n",
              "       0.8117647 , 0.8745098 , 0.85490197, 0.84705883, 0.84705883,\n",
              "       0.6392157 , 0.49803922, 0.4745098 , 0.47843137, 0.57254905,\n",
              "       0.5529412 , 0.34509805, 0.6745098 , 0.25882354, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.00392157, 0.00392157,\n",
              "       0.00392157, 0.        , 0.78431374, 0.9098039 , 0.9098039 ,\n",
              "       0.9137255 , 0.8980392 , 0.8745098 , 0.8745098 , 0.84313726,\n",
              "       0.8352941 , 0.6431373 , 0.49803922, 0.48235294, 0.76862746,\n",
              "       0.8980392 , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.7176471 , 0.88235295, 0.84705883, 0.8745098 , 0.89411765,\n",
              "       0.92156863, 0.8901961 , 0.8784314 , 0.87058824, 0.8784314 ,\n",
              "       0.8666667 , 0.8745098 , 0.9607843 , 0.6784314 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.75686276, 0.89411765,\n",
              "       0.85490197, 0.8352941 , 0.7764706 , 0.7058824 , 0.83137256,\n",
              "       0.8235294 , 0.827451  , 0.8352941 , 0.8745098 , 0.8627451 ,\n",
              "       0.9529412 , 0.7921569 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.00392157, 0.01176471, 0.        ,\n",
              "       0.04705882, 0.85882354, 0.8627451 , 0.83137256, 0.85490197,\n",
              "       0.7529412 , 0.6627451 , 0.8901961 , 0.8156863 , 0.85490197,\n",
              "       0.8784314 , 0.83137256, 0.8862745 , 0.77254903, 0.81960785,\n",
              "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.02352941, 0.        , 0.3882353 , 0.95686275,\n",
              "       0.87058824, 0.8627451 , 0.85490197, 0.79607844, 0.7764706 ,\n",
              "       0.8666667 , 0.84313726, 0.8352941 , 0.87058824, 0.8627451 ,\n",
              "       0.9607843 , 0.46666667, 0.654902  , 0.21960784, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.01568628, 0.        ,\n",
              "       0.        , 0.21568628, 0.9254902 , 0.89411765, 0.9019608 ,\n",
              "       0.89411765, 0.9411765 , 0.9098039 , 0.8352941 , 0.85490197,\n",
              "       0.8745098 , 0.91764706, 0.8509804 , 0.8509804 , 0.81960785,\n",
              "       0.36078432, 0.        , 0.        , 0.        , 0.00392157,\n",
              "       0.01568628, 0.02352941, 0.02745098, 0.00784314, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.92941177,\n",
              "       0.8862745 , 0.8509804 , 0.8745098 , 0.87058824, 0.85882354,\n",
              "       0.87058824, 0.8666667 , 0.84705883, 0.8745098 , 0.8980392 ,\n",
              "       0.84313726, 0.85490197, 1.        , 0.3019608 , 0.        ,\n",
              "       0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.24313726,\n",
              "       0.5686275 , 0.8       , 0.89411765, 0.8117647 , 0.8352941 ,\n",
              "       0.8666667 , 0.85490197, 0.8156863 , 0.827451  , 0.85490197,\n",
              "       0.8784314 , 0.8745098 , 0.85882354, 0.84313726, 0.8784314 ,\n",
              "       0.95686275, 0.62352943, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.07058824, 0.17254902, 0.32156864,\n",
              "       0.41960785, 0.7411765 , 0.89411765, 0.8627451 , 0.87058824,\n",
              "       0.8509804 , 0.8862745 , 0.78431374, 0.8039216 , 0.827451  ,\n",
              "       0.9019608 , 0.8784314 , 0.91764706, 0.6901961 , 0.7372549 ,\n",
              "       0.98039216, 0.972549  , 0.9137255 , 0.93333334, 0.84313726,\n",
              "       0.        , 0.        , 0.22352941, 0.73333335, 0.8156863 ,\n",
              "       0.8784314 , 0.8666667 , 0.8784314 , 0.8156863 , 0.8       ,\n",
              "       0.8392157 , 0.8156863 , 0.81960785, 0.78431374, 0.62352943,\n",
              "       0.9607843 , 0.75686276, 0.80784315, 0.8745098 , 1.        ,\n",
              "       1.        , 0.8666667 , 0.91764706, 0.8666667 , 0.827451  ,\n",
              "       0.8627451 , 0.9098039 , 0.9647059 , 0.        , 0.01176471,\n",
              "       0.7921569 , 0.89411765, 0.8784314 , 0.8666667 , 0.827451  ,\n",
              "       0.827451  , 0.8392157 , 0.8039216 , 0.8039216 , 0.8039216 ,\n",
              "       0.8627451 , 0.9411765 , 0.3137255 , 0.5882353 , 1.        ,\n",
              "       0.8980392 , 0.8666667 , 0.7372549 , 0.6039216 , 0.7490196 ,\n",
              "       0.8235294 , 0.8       , 0.81960785, 0.87058824, 0.89411765,\n",
              "       0.88235295, 0.        , 0.38431373, 0.9137255 , 0.7764706 ,\n",
              "       0.8235294 , 0.87058824, 0.8980392 , 0.8980392 , 0.91764706,\n",
              "       0.9764706 , 0.8627451 , 0.7607843 , 0.84313726, 0.8509804 ,\n",
              "       0.94509804, 0.25490198, 0.28627452, 0.41568628, 0.45882353,\n",
              "       0.65882355, 0.85882354, 0.8666667 , 0.84313726, 0.8509804 ,\n",
              "       0.8745098 , 0.8745098 , 0.8784314 , 0.8980392 , 0.11372549,\n",
              "       0.29411766, 0.8       , 0.83137256, 0.8       , 0.75686276,\n",
              "       0.8039216 , 0.827451  , 0.88235295, 0.84705883, 0.7254902 ,\n",
              "       0.77254903, 0.80784315, 0.7764706 , 0.8352941 , 0.9411765 ,\n",
              "       0.7647059 , 0.8901961 , 0.9607843 , 0.9372549 , 0.8745098 ,\n",
              "       0.85490197, 0.83137256, 0.81960785, 0.87058824, 0.8627451 ,\n",
              "       0.8666667 , 0.9019608 , 0.2627451 , 0.1882353 , 0.79607844,\n",
              "       0.7176471 , 0.7607843 , 0.8352941 , 0.77254903, 0.7254902 ,\n",
              "       0.74509805, 0.7607843 , 0.7529412 , 0.7921569 , 0.8392157 ,\n",
              "       0.85882354, 0.8666667 , 0.8627451 , 0.9254902 , 0.88235295,\n",
              "       0.84705883, 0.78039217, 0.80784315, 0.7294118 , 0.70980394,\n",
              "       0.69411767, 0.6745098 , 0.70980394, 0.8039216 , 0.80784315,\n",
              "       0.4509804 , 0.        , 0.47843137, 0.85882354, 0.75686276,\n",
              "       0.7019608 , 0.67058825, 0.7176471 , 0.76862746, 0.8       ,\n",
              "       0.8235294 , 0.8352941 , 0.8117647 , 0.827451  , 0.8235294 ,\n",
              "       0.78431374, 0.76862746, 0.7607843 , 0.7490196 , 0.7647059 ,\n",
              "       0.7490196 , 0.7764706 , 0.7529412 , 0.6901961 , 0.6117647 ,\n",
              "       0.654902  , 0.69411767, 0.8235294 , 0.36078432, 0.        ,\n",
              "       0.        , 0.2901961 , 0.7411765 , 0.83137256, 0.7490196 ,\n",
              "       0.6862745 , 0.6745098 , 0.6862745 , 0.70980394, 0.7254902 ,\n",
              "       0.7372549 , 0.7411765 , 0.7372549 , 0.75686276, 0.7764706 ,\n",
              "       0.8       , 0.81960785, 0.8235294 , 0.8235294 , 0.827451  ,\n",
              "       0.7372549 , 0.7372549 , 0.7607843 , 0.7529412 , 0.84705883,\n",
              "       0.6666667 , 0.        , 0.00784314, 0.        , 0.        ,\n",
              "       0.        , 0.25882354, 0.78431374, 0.87058824, 0.92941177,\n",
              "       0.9372549 , 0.9490196 , 0.9647059 , 0.9529412 , 0.95686275,\n",
              "       0.8666667 , 0.8627451 , 0.75686276, 0.7490196 , 0.7019608 ,\n",
              "       0.7137255 , 0.7137255 , 0.70980394, 0.6901961 , 0.6509804 ,\n",
              "       0.65882355, 0.3882353 , 0.22745098, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
              "       0.28235295, 0.16078432, 0.13725491, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# BE CURIOUS!\n",
        "# see what the model did\n",
        "x = train_images[0] # instead of a 2D array that is 28*28, it's a single row!\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "WT8xE47SmqDP",
        "outputId": "9b1898f4-f58b-4feb-aeac-6806fa1580fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "784\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0    1    2    3         4         5         6    7         8         9    \\\n",
              "0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
              "1  0.0  0.0  0.0  0.0  0.000000  0.003922  0.000000  0.0  0.000000  0.000000   \n",
              "2  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.086275   \n",
              "3  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.129412  0.376471   \n",
              "4  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
              "5  0.0  0.0  0.0  0.0  0.003922  0.000000  0.000000  0.0  0.000000  0.086275   \n",
              "6  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
              "7  0.0  0.0  0.0  0.0  0.000000  0.003922  0.003922  0.0  0.000000  0.000000   \n",
              "8  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
              "9  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
              "\n",
              "   ...       774       775       776       777       778       779       780  \\\n",
              "0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "1  ...  0.466667  0.447059  0.509804  0.298039  0.000000  0.000000  0.000000   \n",
              "2  ...  0.000000  0.000000  0.003922  0.000000  0.000000  0.000000  0.000000   \n",
              "3  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "4  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "5  ...  0.000000  0.000000  0.000000  0.000000  0.521569  0.654902  0.286275   \n",
              "6  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "7  ...  0.000000  0.000000  0.011765  0.000000  0.321569  0.929412  0.905882   \n",
              "8  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "9  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "       781  782  783  \n",
              "0  0.00000  0.0  0.0  \n",
              "1  0.00000  0.0  0.0  \n",
              "2  0.00000  0.0  0.0  \n",
              "3  0.00000  0.0  0.0  \n",
              "4  0.00000  0.0  0.0  \n",
              "5  0.00000  0.0  0.0  \n",
              "6  0.00000  0.0  0.0  \n",
              "7  0.27451  0.0  0.0  \n",
              "8  0.00000  0.0  0.0  \n",
              "9  0.00000  0.0  0.0  \n",
              "\n",
              "[10 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a96b27e-20b5-4f6b-b208-4c29d43cceb6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.447059</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0.298039</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.129412</td>\n",
              "      <td>0.376471</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.521569</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.286275</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011765</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321569</td>\n",
              "      <td>0.929412</td>\n",
              "      <td>0.905882</td>\n",
              "      <td>0.27451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 784 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a96b27e-20b5-4f6b-b208-4c29d43cceb6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a96b27e-20b5-4f6b-b208-4c29d43cceb6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a96b27e-20b5-4f6b-b208-4c29d43cceb6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# you can see this in the entire dataset too\n",
        "x = pd.DataFrame(train_images)\n",
        "print(x.shape)\n",
        "print(28*28) # see how the shape matches up?\n",
        "x.head(n=10)\n",
        "\n",
        "# that first sample is now just the first row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzPHYJzojcek"
      },
      "source": [
        "We also need to categorically encode the labels, a step which we explain in Chapter 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkDTT9AUjcek"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQxcJD3UnIgB",
        "outputId": "28a53a98-eb96-40ea-90d3-df6337c90b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "# BE CURIOUS! See what it looks like\n",
        "x = train_labels[0]\n",
        "print(x)\n",
        "# check out the output - the first sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "h0VjOu8evA9N",
        "outputId": "c1b98057-ee6b-4c04-d982-1d2a386ce1d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0    1    2    3    4    5    6    7    8    9\n",
              "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
              "1      1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2      1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3      0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4      1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
              "59995  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
              "59996  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "59997  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "59998  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "59999  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[60000 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5bce2584-5bf4-4a42-a0b8-d56dbe23f39c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bce2584-5bf4-4a42-a0b8-d56dbe23f39c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5bce2584-5bf4-4a42-a0b8-d56dbe23f39c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5bce2584-5bf4-4a42-a0b8-d56dbe23f39c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# check them ALL out!\n",
        "x = train_labels\n",
        "x = pd.DataFrame(x)\n",
        "x\n",
        "\n",
        "# so the labels are in an ARRAY with one column for each potential outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avhwTrw-jcem"
      },
      "source": [
        "We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network:\n",
        "we \"fit\" the model to its training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDVcHvbYjcen",
        "outputId": "7532df2c-5ad0-4cc0-e05d-ea3b4611a662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "375/375 [==============================] - 12s 27ms/step - loss: 0.6236 - accuracy: 0.7765 - val_loss: 0.4276 - val_accuracy: 0.8393\n",
            "Epoch 2/30\n",
            "375/375 [==============================] - 10s 27ms/step - loss: 0.4316 - accuracy: 0.8423 - val_loss: 0.4244 - val_accuracy: 0.8452\n",
            "Epoch 3/30\n",
            "375/375 [==============================] - 7s 19ms/step - loss: 0.3859 - accuracy: 0.8590 - val_loss: 0.3821 - val_accuracy: 0.8598\n",
            "Epoch 4/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.3629 - accuracy: 0.8695 - val_loss: 0.3398 - val_accuracy: 0.8777\n",
            "Epoch 5/30\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.3455 - accuracy: 0.8744 - val_loss: 0.3452 - val_accuracy: 0.8805\n",
            "Epoch 6/30\n",
            "375/375 [==============================] - 6s 16ms/step - loss: 0.3309 - accuracy: 0.8789 - val_loss: 0.3428 - val_accuracy: 0.8733\n",
            "Epoch 7/30\n",
            "375/375 [==============================] - 7s 19ms/step - loss: 0.3212 - accuracy: 0.8840 - val_loss: 0.3306 - val_accuracy: 0.8814\n",
            "Epoch 8/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.3095 - accuracy: 0.8865 - val_loss: 0.3692 - val_accuracy: 0.8748\n",
            "Epoch 9/30\n",
            "375/375 [==============================] - 6s 17ms/step - loss: 0.2996 - accuracy: 0.8895 - val_loss: 0.3461 - val_accuracy: 0.8786\n",
            "Epoch 10/30\n",
            "375/375 [==============================] - 9s 25ms/step - loss: 0.2978 - accuracy: 0.8909 - val_loss: 0.3350 - val_accuracy: 0.8828\n",
            "Epoch 11/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2941 - accuracy: 0.8944 - val_loss: 0.3492 - val_accuracy: 0.8900\n",
            "Epoch 12/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2857 - accuracy: 0.8962 - val_loss: 0.3622 - val_accuracy: 0.8878\n",
            "Epoch 13/30\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.2835 - accuracy: 0.8984 - val_loss: 0.3427 - val_accuracy: 0.8855\n",
            "Epoch 14/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2775 - accuracy: 0.8992 - val_loss: 0.3582 - val_accuracy: 0.8829\n",
            "Epoch 15/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2750 - accuracy: 0.9004 - val_loss: 0.3790 - val_accuracy: 0.8855\n",
            "Epoch 16/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2730 - accuracy: 0.9029 - val_loss: 0.3621 - val_accuracy: 0.8909\n",
            "Epoch 17/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2651 - accuracy: 0.9038 - val_loss: 0.3397 - val_accuracy: 0.8932\n",
            "Epoch 18/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2631 - accuracy: 0.9056 - val_loss: 0.3409 - val_accuracy: 0.8878\n",
            "Epoch 19/30\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.2600 - accuracy: 0.9061 - val_loss: 0.3477 - val_accuracy: 0.8928\n",
            "Epoch 20/30\n",
            "375/375 [==============================] - 5s 15ms/step - loss: 0.2583 - accuracy: 0.9079 - val_loss: 0.3830 - val_accuracy: 0.8870\n",
            "Epoch 21/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2528 - accuracy: 0.9078 - val_loss: 0.3605 - val_accuracy: 0.8915\n",
            "Epoch 22/30\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.2505 - accuracy: 0.9089 - val_loss: 0.3732 - val_accuracy: 0.8909\n",
            "Epoch 23/30\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.2483 - accuracy: 0.9087 - val_loss: 0.3844 - val_accuracy: 0.8918\n",
            "Epoch 24/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2461 - accuracy: 0.9109 - val_loss: 0.3869 - val_accuracy: 0.8914\n",
            "Epoch 25/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2448 - accuracy: 0.9122 - val_loss: 0.4406 - val_accuracy: 0.8887\n",
            "Epoch 26/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2430 - accuracy: 0.9139 - val_loss: 0.3982 - val_accuracy: 0.8942\n",
            "Epoch 27/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2424 - accuracy: 0.9131 - val_loss: 0.4266 - val_accuracy: 0.8889\n",
            "Epoch 28/30\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2382 - accuracy: 0.9160 - val_loss: 0.4009 - val_accuracy: 0.8931\n",
            "Epoch 29/30\n",
            "375/375 [==============================] - 6s 16ms/step - loss: 0.2381 - accuracy: 0.9158 - val_loss: 0.4090 - val_accuracy: 0.8942\n",
            "Epoch 30/30\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.2326 - accuracy: 0.9169 - val_loss: 0.4162 - val_accuracy: 0.8916\n"
          ]
        }
      ],
      "source": [
        "# on your own, you are welcome to try including early stopping callbacks\n",
        "history = network.fit(train_images, train_labels, epochs=30, batch_size=128, validation_split=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgJN08f9jceq"
      },
      "source": [
        "Two quantities are being displayed during training: the \"loss\" of the network over the training data, and the accuracy of the network over\n",
        "the training data.\n",
        "\n",
        "We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let's check that our model performs well on the test set too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2M3j05Ojceq",
        "outputId": "cc1d01ad-7639-4756-946c-e4c8f7b606d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4574 - accuracy: 0.8891\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = network.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVotbKXVjces",
        "outputId": "e375e188-8bab-4547-8fa5-ad229c6117e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc: 0.8891000151634216\n"
          ]
        }
      ],
      "source": [
        "print('test_acc:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJU8th_xjcev"
      },
      "source": [
        "\n",
        "Our test set accuracy turns out to be ~98% -- that's quite a bit lower than the training set accuracy.\n",
        "This gap between training accuracy and test accuracy is an example of \"overfitting\",\n",
        "the fact that machine learning models tend to perform worse on new data than on their training data.\n",
        "Overfitting will be a central topic in chapter 3.\n",
        "\n",
        "This concludes our very first example -- you just saw how we could build and a train a neural network to classify handwritten digits, in\n",
        "less than 20 lines of Python code. In the next chapter, we will go in detail over every moving piece we just previewed, and clarify what is really\n",
        "going on behind the scenes. You will learn about \"tensors\", the data-storing objects going into the network, about tensor operations, which\n",
        "layers are made of, and about gradient descent, which allows our network to learn from its training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGiUc2ruvWBK"
      },
      "source": [
        "## Evaluate The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "AbWZYQdGvZcO",
        "outputId": "6d2ea933-6c1f-442d-f3f9-6211f117af27"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# show what the predictions look like for a multi-class classification problem\n",
        "# pretty!\n",
        "\n",
        "# learning curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# loss\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"r\" is for \"solid red line\"\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu2m4Q-kwKsN",
        "outputId": "7f74b13f-90cc-4bb2-8153-db6ca0d8b293"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[889,   0,   8,  41,   4,   2,  50,   0,   6,   0],\n",
              "       [  3, 968,   0,  23,   3,   0,   2,   0,   1,   0],\n",
              "       [ 24,   2, 808,  25,  96,   0,  41,   1,   3,   0],\n",
              "       [ 16,   2,   7, 953,  13,   0,   5,   0,   4,   0],\n",
              "       [  2,   1,  98,  60, 797,   0,  38,   0,   4,   0],\n",
              "       [  0,   0,   0,   0,   0, 960,   0,  22,   1,  17],\n",
              "       [167,   0,  88,  60,  62,   0, 611,   0,  12,   0],\n",
              "       [  0,   0,   0,   0,   0,   7,   0, 975,   0,  18],\n",
              "       [  3,   0,   3,   6,   3,   1,   3,   4, 977,   0],\n",
              "       [  0,   0,   0,   0,   0,   6,   1,  40,   0, 953]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# see how the model did!\n",
        "preds = network.predict(test_images)\n",
        "preds # if you look at preds, there's actually a probability in there...\n",
        "\n",
        "# here it is! almost a perfect prediction\n",
        "# actual is left, predicted is top\n",
        "# names can be found by inspecting Y\n",
        "matrix = confusion_matrix(test_labels.argmax(axis=1), preds.argmax(axis=1))\n",
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5ylyOkwUVz",
        "outputId": "b49634e5-3879-423a-9437-94eed8106935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85      1000\n",
            "           1       0.99      0.97      0.98      1000\n",
            "           2       0.80      0.81      0.80      1000\n",
            "           3       0.82      0.95      0.88      1000\n",
            "           4       0.81      0.80      0.81      1000\n",
            "           5       0.98      0.96      0.97      1000\n",
            "           6       0.81      0.61      0.70      1000\n",
            "           7       0.94      0.97      0.95      1000\n",
            "           8       0.97      0.98      0.97      1000\n",
            "           9       0.96      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_labels.argmax(axis=1), preds.argmax(axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "951y8ZInuvb9"
      },
      "source": [
        "# Try a Baseline Model for Fun\n",
        "Just to show the power on DNNs, why not try a more familiar model like logistic regression? Because it's less accurate and actually takes MORE time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOiiB6Yhwm2m"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LR = LogisticRegression(max_iter=100000000)\n",
        "\n",
        "# warning: takes a long time to fit!\n",
        "\n",
        "# had to make a copy of the labels because it expects a 1D array,\n",
        "# not like the previous to_categorical() we used.\n",
        "clf = LR.fit(train_images, # X\n",
        "             copy_train_labels) # y (stored in original 5, 6, 7, 9 format... original labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viY0yzAvyHbE",
        "outputId": "10c43ebb-aeea-4a2a-9cfb-2627aed4dd5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[806,   2,  11,  53,   4,   2, 111,   0,  11,   0],\n",
              "       [  4, 958,   3,  25,   4,   0,   3,   1,   2,   0],\n",
              "       [ 24,   4, 739,  10, 124,   0,  86,   1,  12,   0],\n",
              "       [ 24,  17,  18, 861,  30,   0,  39,   0,  11,   0],\n",
              "       [  0,   2, 115,  37, 764,   0,  76,   0,   6,   0],\n",
              "       [  0,   0,   0,   1,   0, 922,   0,  48,   7,  22],\n",
              "       [143,   2, 123,  38, 100,   0, 571,   0,  23,   0],\n",
              "       [  0,   0,   0,   0,   0,  35,   0, 939,   0,  26],\n",
              "       [  7,   1,   7,  13,   5,   6,  21,   5, 935,   0],\n",
              "       [  0,   1,   0,   0,   0,  12,   1,  38,   0, 948]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# evaluate confusion matrix\n",
        "preds = clf.predict(test_images)\n",
        "\n",
        "confusion_matrix(copy_test_labels, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuLU56SpyJdZ",
        "outputId": "b63d2fb6-9947-4006-81c6-4b4c305a59ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.80      1000\n",
            "           1       0.97      0.96      0.96      1000\n",
            "           2       0.73      0.74      0.73      1000\n",
            "           3       0.83      0.86      0.84      1000\n",
            "           4       0.74      0.76      0.75      1000\n",
            "           5       0.94      0.92      0.93      1000\n",
            "           6       0.63      0.57      0.60      1000\n",
            "           7       0.91      0.94      0.92      1000\n",
            "           8       0.93      0.94      0.93      1000\n",
            "           9       0.95      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.84      0.84      0.84     10000\n",
            "weighted avg       0.84      0.84      0.84     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# evaluate train results - it does OK,\n",
        "# but NN does better!\n",
        "\n",
        "print(classification_report(copy_test_labels, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LbDymaYzFf0"
      },
      "outputs": [],
      "source": [
        "# interpret the output!\n",
        "# what is going on with the number five?\n",
        "# X is predicted, Y is actual"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}