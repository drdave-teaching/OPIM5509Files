{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/drdave-teaching/OPIM5509Files/blob/main/OPIM5509_Module5_Files/Dave_s_GLoVE_Example.ipynb)"
      ],
      "metadata": {
        "id": "sdymE0Si_jpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "7gTxhV3MYLdb",
        "outputId": "9ef6ddfb-0dec-4aa4-cfd2-320c64829c41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNtlL-NK-yk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "71bbc991-ee4d-4da5-f7e4-623468a20ab8"
      },
      "source": [
        "# link: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "# Example 3: Learning an embedding\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# define documents\n",
        "# EXERCISE: write more here... you can even use a .csv file instad of typing manually...\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "# integer encode the documents\n",
        "vocab_size = 50 # play with this...\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(\"===========================================================\")\n",
        "print(\"THESE ARE THE ENCODED DOCS\")\n",
        "print(\"===========================================================\")\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "# EXERCISE: play with the length of words, more or less... how does it chop them off?\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(\"===========================================================\")\n",
        "print(\"THESE ARE THE PADDED, ONE HOT DOCS\")\n",
        "print(\"===========================================================\")\n",
        "print(padded_docs)\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================================================\n",
            "THESE ARE THE ENCODED DOCS\n",
            "===========================================================\n",
            "[[37, 8], [48, 41], [10, 48], [26, 41], [7], [47], [12, 48], [14, 48], [12, 41], [45, 41, 8, 49]]\n",
            "===========================================================\n",
            "THESE ARE THE PADDED, ONE HOT DOCS\n",
            "===========================================================\n",
            "[[37  8  0  0]\n",
            " [48 41  0  0]\n",
            " [10 48  0  0]\n",
            " [26 41  0  0]\n",
            " [ 7  0  0  0]\n",
            " [47  0  0  0]\n",
            " [12 48  0  0]\n",
            " [14 48  0  0]\n",
            " [12 41  0  0]\n",
            " [45 41  8 49]]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 89.999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT0lta6rNRRN"
      },
      "source": [
        "# this also looks like a great example - visualizing the embeddings is key!\n",
        "# link: https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQn4PPLLLIs1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6eb26ab9-9f0f-4b55-b589-0ec2d6b6aca4"
      },
      "source": [
        "# get access to the weights! since we made them 8-dimensional, each word is 8 dimensional!\n",
        "\n",
        "# link: https://stackoverflow.com/questions/51235118/how-to-get-word-vectors-from-keras-embedding-layer\n",
        "\n",
        "# access the embedding layer through the constructed model\n",
        "# first `0` refers to the position of embedding layer in the `model`\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "\n",
        "# this is the shape of the embeddings\n",
        "print(embeddings.shape)\n",
        "\n",
        "# these are the embeddings\n",
        "print(embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 8)\n",
            "[[ 0.00600388  0.02085383 -0.05811397 -0.07010853  0.09008072  0.02755397\n",
            "  -0.09322599 -0.09458238]\n",
            " [ 0.0338014  -0.03944095 -0.03014462 -0.00836288 -0.03932029  0.04680338\n",
            "   0.01988541 -0.04505447]\n",
            " [ 0.00681535 -0.00395079  0.00187421 -0.03507826 -0.04603286  0.02706984\n",
            "   0.03837525 -0.04505229]\n",
            " [-0.04190177  0.00591962  0.00293298 -0.04113539  0.04500247 -0.00211649\n",
            "   0.02533776 -0.04189419]\n",
            " [ 0.027567   -0.01522393  0.00827737  0.04283581  0.0306324  -0.04677843\n",
            "  -0.03341582  0.00299717]\n",
            " [-0.01506797 -0.03601909 -0.03489845  0.03185412  0.00814904 -0.01588036\n",
            "  -0.03804338  0.035438  ]\n",
            " [ 0.03422118 -0.04536521 -0.03663887 -0.01247614  0.02244287 -0.02443531\n",
            "   0.01696977  0.01002834]\n",
            " [ 0.05501074  0.06826174  0.0795456   0.07989514 -0.05112276 -0.01322161\n",
            "  -0.10497637  0.07594394]\n",
            " [-0.04181968 -0.03773943 -0.00233484  0.04760867 -0.08919214 -0.05641707\n",
            "  -0.06280068 -0.01243507]\n",
            " [-0.01222948  0.01491017 -0.04054014  0.03539288 -0.04779878  0.0332469\n",
            "  -0.00990149 -0.02539843]\n",
            " [ 0.06586032  0.03129642  0.00576032  0.08857322 -0.01446619 -0.01284298\n",
            "  -0.07551441  0.03168668]\n",
            " [ 0.04347384 -0.01001728 -0.02068084 -0.02701452 -0.00657368 -0.02991234\n",
            "   0.00077704  0.04224641]\n",
            " [-0.03032917 -0.09285417 -0.08209477 -0.09165812  0.00537761  0.08028018\n",
            "   0.10765406 -0.08071836]\n",
            " [ 0.02259493 -0.03379688  0.04162339 -0.00282208  0.0343727   0.03993441\n",
            "  -0.04325383 -0.03477975]\n",
            " [-0.04312249 -0.06070929 -0.10013086 -0.05133977  0.02418854  0.02658691\n",
            "   0.06855237 -0.08329861]\n",
            " [-0.02441446 -0.01611079  0.03127216  0.04466    -0.04160555  0.02604885\n",
            "  -0.04653691 -0.03037672]\n",
            " [-0.03103471 -0.03114141 -0.04456556  0.00530006 -0.01106628 -0.02059708\n",
            "   0.04783133  0.03381786]\n",
            " [ 0.02685281  0.03574354  0.01772423  0.02400203 -0.04118429 -0.01783689\n",
            "  -0.04921346 -0.04272607]\n",
            " [-0.03540606 -0.02051712  0.00223292  0.01591628  0.01937452 -0.0290094\n",
            "  -0.02437511  0.03435259]\n",
            " [-0.02463219  0.01941258 -0.02465814 -0.00775987  0.03889245 -0.0034988\n",
            "   0.00888568  0.00593914]\n",
            " [-0.04063265 -0.02075547 -0.04325959 -0.02587019 -0.04456098 -0.00206816\n",
            "   0.00537865 -0.03852925]\n",
            " [-0.03172987 -0.02704213  0.02717295  0.00485309  0.03181267 -0.04375413\n",
            "  -0.013916    0.00045419]\n",
            " [-0.01364266 -0.02934719 -0.03687718 -0.02300604  0.03056442  0.00837832\n",
            "  -0.02784442 -0.03481643]\n",
            " [ 0.01817778 -0.01020863  0.0441646   0.00589994  0.01990502  0.02598559\n",
            "   0.02104529 -0.00730855]\n",
            " [-0.01375693 -0.03884944 -0.03600216  0.03462695 -0.02425674  0.0065667\n",
            "   0.04236802 -0.00162517]\n",
            " [-0.00990658 -0.01507442 -0.02382087  0.03766893  0.0473926   0.01247429\n",
            "   0.01617146 -0.04838414]\n",
            " [ 0.08997083  0.0793635   0.061088    0.0978019  -0.02517049 -0.08988432\n",
            "  -0.04399066  0.0306614 ]\n",
            " [-0.0297303   0.0335036  -0.00745609  0.02787871  0.00201176  0.02766362\n",
            "  -0.017934    0.00884438]\n",
            " [-0.00262266  0.02862985 -0.02104451 -0.02219497  0.0423293  -0.01482692\n",
            "  -0.0032985   0.00415152]\n",
            " [-0.02078679 -0.02911847  0.01753816 -0.02907622  0.0394617  -0.03696869\n",
            "  -0.02604723 -0.01099724]\n",
            " [ 0.04932973 -0.01082341 -0.00389028 -0.00197405 -0.01919203  0.04209436\n",
            "  -0.04296309 -0.03195398]\n",
            " [ 0.03307972 -0.04036059 -0.04475686 -0.00343372 -0.0214337  -0.03942245\n",
            "   0.01064607  0.02773123]\n",
            " [ 0.04950957  0.01241914  0.04778733  0.03499809 -0.03520373 -0.03616611\n",
            "   0.02839455  0.00043405]\n",
            " [ 0.04142417 -0.04892607 -0.03077315 -0.03480688 -0.00032793 -0.00111616\n",
            "  -0.00116108 -0.0352959 ]\n",
            " [ 0.04602985  0.01445813  0.00545962  0.00603942 -0.02537164  0.04584457\n",
            "   0.04338105 -0.03723725]\n",
            " [-0.04590491 -0.03905052 -0.01767614  0.03106774 -0.03771138  0.00544645\n",
            "  -0.02379134  0.02662626]\n",
            " [-0.00271177 -0.0214026   0.03105957 -0.04365139  0.0188825  -0.00665582\n",
            "  -0.04995323 -0.00696249]\n",
            " [ 0.02012518  0.09530072  0.00405434  0.08124936 -0.05692536 -0.07229022\n",
            "  -0.01055744  0.02870583]\n",
            " [ 0.04063742  0.03208735  0.00742716 -0.00639195  0.04178237 -0.031367\n",
            "  -0.0490188  -0.04081057]\n",
            " [ 0.0055487   0.03301641 -0.03942258  0.00357118  0.0487303   0.00349373\n",
            "  -0.00837117  0.04154191]\n",
            " [ 0.01903038 -0.04767709 -0.04235099  0.02305866  0.0291662   0.03422374\n",
            "  -0.03732026  0.04849039]\n",
            " [ 0.04169979  0.00384715  0.00385891  0.00996753  0.02894402 -0.01882345\n",
            "  -0.00488474  0.03510128]\n",
            " [ 0.01538534 -0.02597569 -0.02348958 -0.0333      0.02287601  0.02802986\n",
            "   0.04377821 -0.04942816]\n",
            " [-0.00528198 -0.00122821 -0.04341228  0.01059272 -0.00090046 -0.03157575\n",
            "   0.02510535 -0.00549515]\n",
            " [-0.01685657  0.0286761  -0.04857994  0.0329374  -0.02572916  0.03344425\n",
            "  -0.03069345 -0.03701723]\n",
            " [-0.08314807 -0.02211502 -0.08208036 -0.06163088  0.0914035   0.00832662\n",
            "   0.04112155 -0.08750582]\n",
            " [-0.0108817  -0.0487538  -0.0065253  -0.03009306  0.02033497 -0.00977594\n",
            "  -0.0455553  -0.04841664]\n",
            " [-0.05772418 -0.01273788 -0.00829372 -0.06959777  0.05112644  0.07948403\n",
            "   0.09942832 -0.05992866]\n",
            " [ 0.02848002  0.03469611  0.02627917  0.03485338 -0.0050161  -0.0452854\n",
            "  -0.08838681  0.06218517]\n",
            " [ 0.04155985 -0.03986973  0.1008855   0.08685564 -0.05005568  0.00919455\n",
            "   0.03983682  0.02249724]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krdCF7NRm-ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c59dcbfc-715b-4fa7-bd4a-ae805fdbe1f4"
      },
      "source": [
        "# do a lookup for each word\n",
        "embeddings[34]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.04602985,  0.01445813,  0.00545962,  0.00603942, -0.02537164,\n",
              "        0.04584457,  0.04338105, -0.03723725], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Link to the glove.6B.100d.txt file on Github\n",
        "# \"https://github.com/drdave-teaching/OPIM5509Files/raw/refs/heads/main/OPIM5509_Module5_Files/glove.6B.100d.txt\""
      ],
      "metadata": {
        "id": "fyV6av1iLG7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1OZ9fiJCZV1mIpGvw_mS4ZJKZS-QaoJ5S/view?usp=sharing\n",
        "!gdown 1OZ9fiJCZV1mIpGvw_mS4ZJKZS-QaoJ5S"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dySCXedXk6L",
        "outputId": "d874785b-a491-4924-e15d-30cd9372de40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1OZ9fiJCZV1mIpGvw_mS4ZJKZS-QaoJ5S\n",
            "From (redirected): https://drive.google.com/uc?id=1OZ9fiJCZV1mIpGvw_mS4ZJKZS-QaoJ5S&confirm=t&uuid=432f0b87-8da3-466a-9d99-50827c2c40f1\n",
            "To: /content/glove.6B.100d.txt\n",
            "100% 347M/347M [00:09<00:00, 37.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRrK4Q_VX_jZ",
        "outputId": "91c91d8f-a0f4-496f-b6d4-9347f3aad8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCl3nJgFqIkN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "519066fc-d224-4603-fbd5-f18c1da402d1"
      },
      "source": [
        "# now let's try using GLoVE (make sure this is on your computer!) - this can go into a different script?\n",
        "# link: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "# load the whole embedding into memory\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embeddings_index = dict()\n",
        "\n",
        "f = open(\"/content/glove.6B.100d.txt\")\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "# prepare tokenizer - we can break each out into a different cell in class\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "# define model\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n",
            "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
            "[[ 6  2  0  0]\n",
            " [ 3  1  0  0]\n",
            " [ 7  4  0  0]\n",
            " [ 8  1  0  0]\n",
            " [ 9  0  0  0]\n",
            " [10  0  0  0]\n",
            " [ 5  4  0  0]\n",
            " [11  3  0  0]\n",
            " [ 5  1  0  0]\n",
            " [12 13  2 14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │           \u001b[38;5;34m1,500\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,500\u001b[0m (5.86 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> (5.86 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,500\u001b[0m (5.86 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> (5.86 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Accuracy: 100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g08Xac0_vsn7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "26f351d2-4a0b-4396-a453-d7b6130f27c5"
      },
      "source": [
        "# look at the embedding matrix\n",
        "print(embedding_matrix.shape) # 15 words, each are 100 dimensions\n",
        "\n",
        "# these are all of the words and their index... you can look back at the table!\n",
        "print(t.word_index.items())\n",
        "\n",
        "# in comparison, this is the ordered dictionary - word and frequency associated with it!\n",
        "print(t.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15, 100)\n",
            "dict_items([('work', 1), ('done', 2), ('good', 3), ('effort', 4), ('poor', 5), ('well', 6), ('great', 7), ('nice', 8), ('excellent', 9), ('weak', 10), ('not', 11), ('could', 12), ('have', 13), ('better', 14)])\n",
            "OrderedDict([('well', 1), ('done', 2), ('good', 2), ('work', 3), ('great', 1), ('effort', 2), ('nice', 1), ('excellent', 1), ('weak', 1), ('poor', 2), ('not', 1), ('could', 1), ('have', 1), ('better', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg8DHYDUzFuA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "32727aee-cf5c-4c46-a6aa-594a6240455f"
      },
      "source": [
        "# this is the embedding associated with each one!\n",
        "# note how it goes from 1 to 14, does not include 0 or 15\n",
        "# you can verify this by looking at t.word_index.items()\n",
        "\n",
        "# this is the word embedding for \"work\"\n",
        "embedding_matrix[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.16190001e-01,  4.54470009e-01, -6.92160010e-01,  3.45799997e-02,\n",
              "        2.63480008e-01, -3.81390005e-01, -2.27899998e-01,  3.72330010e-01,\n",
              "       -2.05789998e-01,  2.90199995e-01,  1.21140003e-01, -4.27289993e-01,\n",
              "        5.55729985e-01, -9.42860022e-02, -4.99669999e-01, -2.94779986e-01,\n",
              "        7.41090000e-01,  2.51910001e-01, -2.74679989e-01,  2.31910005e-01,\n",
              "        3.82039999e-03,  4.52519991e-02,  2.49699995e-01, -4.15789992e-01,\n",
              "        3.13069999e-01, -5.84959984e-01, -3.27389985e-01, -6.61889970e-01,\n",
              "        1.49090007e-01, -2.57710010e-01, -9.48580027e-01,  4.18089986e-01,\n",
              "       -2.95379996e-01, -4.27110009e-02, -6.99699998e-01,  5.78920007e-01,\n",
              "       -6.92709982e-02, -3.96329984e-02, -5.64630004e-03, -2.96160012e-01,\n",
              "       -5.74479997e-01,  1.60099998e-01, -1.06710002e-01,  1.00960001e-01,\n",
              "       -4.29560006e-01, -2.77850002e-01, -3.00170004e-01, -6.95370018e-01,\n",
              "        1.79649994e-01, -4.67229992e-01,  1.25110000e-01, -2.90220007e-02,\n",
              "       -1.59740001e-01,  1.42170000e+00, -2.62239993e-01, -2.37190008e+00,\n",
              "       -1.19170003e-01, -5.82620025e-01,  1.55480003e+00,  4.22120005e-01,\n",
              "        8.46330002e-02,  1.13849998e+00, -3.12260002e-01, -5.07379994e-02,\n",
              "        1.09360003e+00, -1.43700000e-03,  4.46410000e-01,  3.46249998e-01,\n",
              "        4.39639986e-01, -3.89409989e-01,  2.70819992e-01,  8.06260034e-02,\n",
              "       -5.64810000e-02, -4.10970002e-01,  5.64199984e-01, -1.51580006e-01,\n",
              "       -1.49309993e-01,  1.71990007e-01, -6.14950001e-01, -1.58219993e-01,\n",
              "        3.28179985e-01,  4.97559994e-01, -3.29470009e-01,  1.62220001e-01,\n",
              "       -2.30629992e+00,  4.06809986e-01,  2.77020007e-01, -3.70020002e-01,\n",
              "       -8.09689999e-01, -7.62560010e-01,  9.11090001e-02, -6.14730000e-01,\n",
              "        2.52860010e-01,  5.99800013e-02, -2.19769999e-01,  7.23819993e-03,\n",
              "       -3.38770002e-01, -5.47370017e-01,  4.88220006e-01,  3.22459996e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9xs1Aq0KMA"
      },
      "source": [
        "# we can also use a simpleRNN\n",
        "# and we can also apply these to the IMDB dataset (tomorrow!)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}